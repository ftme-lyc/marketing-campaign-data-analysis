# -*- coding: utf-8 -*-
"""Statistical_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZW3PRPGHGmHs_BaKbkKDqB2ItidPs11
"""

#!pip install dataprep

"""Data Cleaning"""

import pandas as pd
import numpy as np
from io import StringIO
import dataprep
from dataprep.eda import plot
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RandomizedSearchCV, RandomizedSearchCV, train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve
import time
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, roc_auc_score, roc_curve, RocCurveDisplay, confusion_matrix, classification_report, precision_score, recall_score
from imblearn.over_sampling import SMOTE
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.optimize import minimize

# Importing the Dataset
with open('TeleCom_Data (1).csv', 'r') as file:
  data = file.read().replace('"', '')
telecom_df = pd.read_csv(StringIO(data), sep=';')
telecom_df

# Column Descriptions
# telecom_df.info()
telecom_df.describe(include='all')

# Check for Duplicate Rows
duplicate_rows = telecom_df.duplicated().sum()
print(f"Number of duplicate rows before removing: {duplicate_rows}")
df = telecom_df.copy()
df = df.drop_duplicates()
df_d_rows = df.duplicated().sum()
print(f"Number of duplicate rows after removing: {df_d_rows}")
df.shape

# Check for Missing Values
df.isnull().sum()

# Check for Invalid Data Entries
df['age'].unique()
df['marital'].unique()
df['marital'].value_counts() # 80 unknown
df['job'].value_counts() # 330 unknown
df['education'].value_counts() # 1730 unknown
df['default'].value_counts() # 8595 unknown
df['housing'].value_counts() # 990 unknown
df['loan'].value_counts() # 990 unknown
df['contact'].value_counts()
df['month'].value_counts()
df['day_of_week'].value_counts()
df['duration'].value_counts()
df['campaign'].value_counts()
df['pdays'].value_counts()
df['previous'].value_counts()
df['poutcome'].value_counts() # 35547 nonexistent
df['emp.var.rate'].value_counts()
df['cons.price.idx'].value_counts()
df['cons.conf.idx'].value_counts()
df['euribor3m'].value_counts()
df['nr.employed'].value_counts()
df['y'].value_counts()

# Check for the Presence of Invalid Values
df[df['age']<0]
df[df['duration'] < 0]
df[df['campaign'] < 0]
df[df['pdays'] < 0]
df[df['previous'] < 0]

# Replace Missing or Invalid Values

df['marital'].mode() # married
df['marital'].replace('unknown', 'married', inplace=True)

df['job'].mode() # admin.
df['job'].replace('unknown', 'admin.', inplace=True)

for idx, row in df[df['education'] == 'unknown'].iterrows():
    age = row['age']

    if age < 18:
        df.at[idx, 'education'] = 'high.school'
    elif 18 <= age < 25:
        df.at[idx, 'education'] = 'university.degree'
    else:
        df.at[idx, 'education'] = 'professional.course'

df['housing'].mode() # yes
df['housing'].replace('unknown', 'yes', inplace=True)

df['loan'].mode() # no
df['loan'].replace('unknown', 'no', inplace=True)

df.head()

df_copy = df.copy()

le = LabelEncoder()
df_copy['job'] = le.fit_transform(df_copy['job'])
df_copy['contact'] = le.fit_transform(df_copy['contact'])
df_copy['marital'] = le.fit_transform(df_copy['marital'])
mapping = {'illiterate': 0,
           'basic.4y' : 1,
           'basic.6y' : 2,
           'basic.9y' : 3,
           'high.school' : 4,
           'university.degree' : 5,
           'professional.course' : 6}

mapping1 = {'no': 0,'yes' : 1,'unknown' : -1}
mapping2 = {'no': 0,'yes' : 1,}
mapping3 = {'mon' : 1,'tue' : 2,'wed' : 3,'thu' : 4,'fri' : 5}
mapping4 = {'jan' : 1,'feb' : 2,'mar' : 3,'apr' : 4,'may': 5,'jun' : 6,'jul' : 7,'aug' : 8,'sep' : 9,'oct' : 10,'nov' : 11,'dec' : 12}
mapping5 = {'failure': 0,'success' : 1,'nonexistent' : -1}

df_copy['education'] = df['education'].map(mapping)
df_copy['default'] = df['default'].map(mapping1)
df_copy['housing'] = df['housing'].map(mapping2)
df_copy['loan'] = df['loan'].map(mapping2)
df_copy['y'] = df['y'].map(mapping2)
df_copy['day_of_week'] = df['day_of_week'].map(mapping3)
df_copy['month'] = df['month'].map(mapping4)
df_copy['poutcome'] = df['poutcome'].map(mapping5)

# dummies = pd.get_dummies(df_copy['marital'])
# encoded_df = pd.concat([df_copy, dummies], axis=1)
# encoded_df.drop(columns=['marital'], inplace=True)

encoded_df = df_copy.copy()

"""Data Visualisation"""

# check for outliers
# Descriptive statistics

plot(df, 'age')

plot(df, 'education')

plot(df, 'previous')

plot(df, 'duration')

# Bivariate Plots
fig , ax = plt.subplots(1, 3, figsize = (20,5))
ax[0].bar(df['month'], df['campaign'])
ax[1].bar(df['day_of_week'], df['campaign'])
ax[2].scatter(df['cons.conf.idx'], df['cons.price.idx'])
ax[0].set_title("Month vs. Campaign")
ax[1].set_title("Day vs. Campaign")
ax[2].set_title("Cons.conf.idx vs. Cons.price.idx")
ax[0].set_xlabel("month")
ax[0].set_ylabel("campaign")
ax[1].set_xlabel("day")
ax[1].set_ylabel("campaign")
ax[2].set_xlabel("cons.conf.idx")
ax[2].set_ylabel("cons.price.idx")
plt.show()

# Scatter Matrix
matrix = scatter_matrix(df,figsize=(12,12))

# Count Plot: Job vs. y
sns.set(style="darkgrid")
plt.figure(figsize=(12, 6))
sns.countplot(x='job', hue='y', data=df, palette='Set2')
plt.title('Job vs. Response Variable (y)')
plt.xlabel('job')
plt.xticks(rotation=45)
plt.ylabel('count (y)')
plt.legend(title='y')
plt.show()

# Distribution of Positive and Negative Responses Across Different Job Categories
job_counts = df.groupby(['job', 'y']).size().unstack(fill_value=0)
job_percentage = job_counts.div(job_counts.sum(axis=1), axis=0) * 100
job_percentage_sorted = job_percentage.sort_values(by='yes',ascending=False)
job_percentage_sorted

# Box Plot: Age vs. y
plt.figure(figsize=(12, 6))
sns.boxplot(x='y', y='age', data=df, palette="Set2")
plt.title('Age vs. Response Variable (y)')
plt.xlabel('y')
plt.ylabel('age')
plt.show()

# Distribution of Positive and Negative Responses Across Different Age Groups
Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1
df_no_outliers = df[~((df['age'] < (Q1 - 1.5 * IQR)) | (df['age'] > (Q3 + 1.5 * IQR)))]
age_counts = df_no_outliers.groupby(['age', 'y']).size().unstack(fill_value=0)
age_percentage = age_counts.div(age_counts.sum(axis=1), axis=0) * 100
age_percentage
age_percentage_sorted = age_percentage.sort_values(by='yes',ascending=False)
age_percentage_sorted.head(10)

# Bar Plot: Education vs. y
plt.figure(figsize=(12, 6))
sns.countplot(x='education', hue='y', data=df, palette="Set2")
plt.title('Education vs. Response Variable (y)')
plt.xlabel('education')
plt.ylabel('count (y)')
plt.legend(title='y')
plt.show()

# Box Plot: Duration vs. y
plt.figure(figsize=(12, 6))
sns.boxplot(x='y', y='duration', data=df, palette="Set2")
plt.title('Duration vs. Response Variable (y)')
plt.xlabel('y')
plt.ylabel('duration')
plt.show()

# Violin Plot: Job/Age vs. y
plt.figure(figsize=(12, 6))
sns.violinplot(x='job', y='age', data=df, hue="y", palette="Set2")
plt.title('Job/Age vs. Response Variable (y)')
plt.xticks(rotation=45)
plt.xlabel('job')
plt.ylabel('age')
plt.show()

# Scatter plot: Age vs. Current Calls
sns.scatterplot(x='age', y='campaign', data=df)
plt.title('Age vs. Number of Current Calls')
plt.xlabel('age')
plt.ylabel('number of current calls')
plt.show()

encoded_df

encoded_df.corr()

sns.heatmap(df_copy.corr(), cmap='coolwarm')

"""PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.3, random_state=42)
X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
def plot_pca_variance(pca):
    plt.figure(figsize=(8, 6))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA: Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

pca = PCA()
pca.fit(X_scaled)
plot_pca_variance(pca)
# Plot the PCA explained variance
n_components=16
pca = PCA(n_components=n_components)
#print(f"Applying PCA with {n_components} components...")
pca = PCA(n_components=n_components, random_state=42)
X_pca = pca.fit_transform(X_scaled)
plot_pca_variance(pca)

pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
pca_df = pd.DataFrame(X_pca, columns=pca_columns)
pca_df['y'] = encoded_df['y'].values
pca_df.head()

"""GLM using MLE"""

# Assuming 'df' is the dataset with the response variable 'y' and other predictors
response = 'y'  # Replace with your actual response variable
predictors = encoded_df.columns.drop(response)  # Exclude the response column from predictors

# Split data into X (predictors) and y (target)
X = encoded_df[predictors]
y = encoded_df[response]

# Initialize cross-validation scheme
n_folds = 5
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

# Dictionary to store results for each model
results = {"GLM": [], "Logistic Regression": [], "Decision Tree": [], "Random Forest": []}


# 1. GLM Model (Parametric)
params_glm = []
validation_scores_glm = []
auc_scores_glm = []

for train_index, test_index in kf.split(encoded_df):
    train = encoded_df.iloc[train_index]
    test = encoded_df.iloc[test_index]

    X_train = sm.add_constant(train[predictors])
    X_valid = sm.add_constant(test[predictors])
    y_train = train[response]
    y_valid = test[response]
    X_train = X_train.apply(pd.to_numeric, errors='coerce')
    X_valid = X_valid.apply(pd.to_numeric, errors='coerce')

    # GLM with Binomial family
    glm_model = sm.GLM(y_train, X_train, family=sm.families.Binomial())
    glm_result = glm_model.fit()

    # Store validation AUC
    y_pred_glm = glm_result.predict(X_valid)
    auc_score_glm = roc_auc_score(y_valid, y_pred_glm)
    auc_scores_glm.append(auc_score_glm)
    validation_scores_glm.append(mean_squared_error(y_valid, y_pred_glm))

# Store GLM results
results['GLM'] = {
    "mean_auc_score": np.mean(auc_scores_glm),
    "mean_validation_score": np.mean(validation_scores_glm),
    "auc_scores_per_fold": auc_scores_glm
}



print(glm_result.summary())

# Create a DataFrame for validation scores
validation_scores_df = pd.DataFrame({"Fold": range(1, n_folds + 1), "Validation Score": validation_scores_glm})

# Print the validation scores table
print("\nValidation Scores:")
print(validation_scores_df)

def train_model_with_random_search(model, param_grid, X_train, y_train, X_test, y_test, cv=5, n_iter=50):
    """
    Train a model with RandomizedSearchCV, evaluate accuracy, F1-score, and ROC-AUC, and plot ROC curve.

    Parameters:
    model: The machine learning model (e.g., LogisticRegression, DecisionTree, etc.)
    param_grid: The parameter grid for RandomizedSearchCV
    X_train, y_train: Training data and labels
    X_test, y_test: Testing data and labels
    cv: Cross-validation folds
    n_iter: Number of iterations for RandomizedSearchCV

    Returns:
    model: The trained model with the best parameters
    acc: Accuracy of the model on the test set
    f1: F1 score of the model on the test set
    roc_auc: ROC AUC score on the test set, or 0 if unavailable
    best_params: Best parameters found by RandomizedSearchCV, or an empty dict if unavailable
    """
    # Initialize RandomizedSearchCV
    random_search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, scoring='accuracy', cv=cv, random_state=42, n_jobs=-1)

    # Train the model using RandomizedSearchCV
    random_search.fit(X_train, y_train)

    # Get the best model and parameters
    best_model = random_search.best_estimator_
    best_params = random_search.best_params_ if random_search.best_params_ else {}

    # Make predictions
    y_pred = best_model.predict(X_test)

    # Check if the model has a predict_proba method (for ROC-AUC)
    if hasattr(best_model, "predict_proba"):
        y_prob = best_model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_prob)
    else:
        y_prob = None
        roc_auc = 0  # Default value if predict_proba is unavailable

    # Calculate metrics
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Print results
    print(f"Best Parameters: {best_params}")
    print(f"Accuracy: {acc}")
    print(f"F1 Score: {f1}")
    print(f"ROC AUC Score: {roc_auc}")

    # Plot the ROC curve if ROC-AUC was computed
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc='lower right')
        plt.show()

    return best_model, acc, f1, roc_auc, best_params

"""XGBOOST Model without Smote or PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

xgb_param_grid = {
    'scale_pos_weight': [1, 5, 10],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 5, 10],
    'gamma': [0, 0.1, 0.3],
    'subsample': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.2]
}

model_xgb = XGBClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_xgb, xgb_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# XGBoost Model
xgb_model = XGBClassifier(subsample= 0.6, scale_pos_weight= 1, min_child_weight= 1, max_depth= 3, learning_rate= 0.2, gamma= 0.3)
xgb_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = xgb_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = xgb_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""XGBoost Model using Smote without PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

xgb_param_grid = {
    'scale_pos_weight': [1, 5, 10],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 5, 10],
    'gamma': [0, 0.1, 0.3],
    'subsample': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.2]
}

model_xgb = XGBClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_xgb, xgb_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# XGBoost Model
xgb_model = XGBClassifier(subsample= 0.6, scale_pos_weight= 1, min_child_weight= 5, max_depth= 7, learning_rate= 0.2, gamma= 0.1)
xgb_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = xgb_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = xgb_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""XGBoost Model using Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

def plot_pca_variance(pca):
    plt.figure(figsize=(8, 6))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA: Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

n_components=17
pca=PCA().fit(X_scaled)
plot_pca_variance(pca)

print(f"Applying PCA with {n_components} components...")
pca = PCA(n_components=n_components, random_state=42)
X_pca = pca.fit_transform(X_scaled)
# Plot the PCA explained variance
plot_pca_variance(pca)
pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
pca_df = pd.DataFrame(X_pca, columns=pca_columns)
pca_df.head()

X_train, X_test, y_train, y_test = train_test_split(pca_df, y, test_size=0.2)

xgb_param_grid = {
    'scale_pos_weight': [1, 5, 10],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 5, 10],
    'gamma': [0, 0.1, 0.3],
    'subsample': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.2]
}

model_xgb = XGBClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_xgb, xgb_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# XGBoost Model
xgb_model = XGBClassifier(subsample= 0.8, scale_pos_weight= 1, min_child_weight= 10, max_depth= 7, learning_rate= 0.2, gamma= 0)
xgb_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = xgb_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = xgb_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""MLP Model without Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)
mlp_param_grid = {
    'hidden_layer_sizes': [(10,10,10), (10,20,10), (20,)],
    'activation': ['logistic', 'relu'],
    'solver': ['adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['adaptive'],
}

model_mlp = MLPClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_mlp, mlp_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# MLP Model
mlp_model = MLPClassifier(solver='adam', hidden_layer_sizes=(10, 10, 10), alpha=0.05, activation='relu', learning_rate='adaptive')
mlp_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(mlp_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = mlp_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = mlp_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""MLP Model using Smote without PCA"""

# Data Preparation
X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

# Applying SMOTE on the training data
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Combine the SMOTE-applied training data for scaling
X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Define MLP Model in PyTorch
class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_layer_sizes):
        super(SimpleMLP, self).__init__()
        layers = []
        in_size = input_size
        for hidden_size in hidden_layer_sizes:
            layers.append(nn.Linear(in_size, hidden_size))
            layers.append(nn.ReLU())
            in_size = hidden_size
        layers.append(nn.Linear(in_size, 1))  # Output layer for binary classification
        layers.append(nn.Sigmoid())  # Sigmoid for binary classification
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Custom MLE (Cross-Entropy) Loss Function
def log_likelihood_mle(output, target):
    epsilon = 1e-9
    likelihood = target * torch.log(output + epsilon) + (1 - target) * torch.log(1 - output + epsilon)
    return -torch.mean(likelihood)

# Hyperparameter Tuning Function
def tune_and_train_mlp(X_train, y_train, X_test, y_test, param_grid, num_epochs=100):
    best_model = None
    best_params = None
    best_acc = 0

    for hidden_layer_sizes in param_grid['hidden_layer_sizes']:
        for alpha in param_grid['alpha']:
            model = SimpleMLP(input_size=X_train.shape[1], hidden_layer_sizes=hidden_layer_sizes)
            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=alpha)

            for epoch in range(num_epochs):
                model.train()
                optimizer.zero_grad()

                output = model(X_train)
                loss = log_likelihood_mle(output, y_train)
                loss.backward()
                optimizer.step()

            # Evaluate on test data
            model.eval()
            with torch.no_grad():
                y_pred_prob = model(X_test)
                y_pred = (y_pred_prob > 0.5).float()
                acc = accuracy_score(y_test, y_pred)

                # Update the best model if accuracy improves
                if acc > best_acc:
                    best_acc = acc
                    best_model = model
                    best_params = {
                        'hidden_layer_sizes': hidden_layer_sizes,
                        'alpha': alpha
                    }

    return best_model, best_params, best_acc

# Parameter Grid
param_grid = {
    'hidden_layer_sizes': [(10, 10, 10), (10, 20, 10), (20,)],
    'alpha': [0.0001, 0.05]
}

# Train and Tune the Model
best_model, best_params, best_acc = tune_and_train_mlp(X_train, y_train, X_test, y_test, param_grid)
print(f'Best Parameters: {best_params}')
print(f'Best Accuracy: {best_acc}')

# Evaluate the Best Model
best_model.eval()
with torch.no_grad():
    y_pred_prob = best_model(X_test)
    y_pred = (y_pred_prob > 0.5).float()
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_prob)

    print("Test Accuracy:", acc)
    print("F1 Score:", f1)
    print("ROC AUC Score:", roc_auc)
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

mlp_param_grid = {
    'hidden_layer_sizes': [(10,10,10), (10,20,10), (20,)],
    'activation': ['logistic', 'relu'],
    'solver': ['adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['adaptive'],
}

model_mlp = MLPClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_mlp, mlp_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# MLP Model
mlp_model = MLPClassifier(solver='adam', hidden_layer_sizes=(10, 20, 10), alpha=0.0001, activation='relu', learning_rate='adaptive')
mlp_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(mlp_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = mlp_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = mlp_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""MLP Model using Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

def plot_pca_variance(pca):
    plt.figure(figsize=(8, 6))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA: Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

n_components=17
pca=PCA().fit(X_scaled)
plot_pca_variance(pca)

print(f"Applying PCA with {n_components} components...")
pca = PCA(n_components=n_components, random_state=42)
X_pca = pca.fit_transform(X_scaled)
# Plot the PCA explained variance
plot_pca_variance(pca)
pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
pca_df = pd.DataFrame(X_pca, columns=pca_columns)
pca_df.head()

X_train, X_test, y_train, y_test = train_test_split(pca_df, y, test_size=0.2)

mlp_param_grid = {
    'hidden_layer_sizes': [(10,10,10), (10,20,10), (20,)],
    'activation': ['logistic', 'relu'],
    'solver': ['adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['adaptive'],
}

model_mlp = MLPClassifier()

time_start = time.time()
model_mlp, acc_mlp, f1_mlp, roc_auc_mlp, param_mlp = train_model_with_random_search(model_mlp, mlp_param_grid,
                                                                         X_train, y_train, X_test, y_test, cv=5)
time_taken_mlp = time.time() - time_start
print('Best Parameters: ', param_mlp)
print("Time Taken: ", round(time_taken_mlp, 2), ' Seconds')

# MLP Model
mlp_model = MLPClassifier(solver='adam', hidden_layer_sizes=(10, 20, 10), alpha=0.05, activation='relu', learning_rate='adaptive')
mlp_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(mlp_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = mlp_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = mlp_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Decision Tree Model without Smote & PCA

"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)
dt_param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model_dt = DecisionTreeClassifier()

time_start = time.time()
model_dt, acc_dt, f1_dt, roc_auc_dt, param_dt = train_model_with_random_search(model_dt, dt_param_grid,
                                                                    X_train, y_train, X_test, y_test, cv=5)
time_taken_dt = time.time() - time_start
print('Best Parameters: ', param_dt)
print("Time Taken: ", round(time_taken_dt, 2), ' Seconds')

# Decision Tree Model
X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

dt_model = DecisionTreeClassifier(splitter = 'best', min_samples_split = 2, min_samples_leaf = 4, max_depth = 10, criterion = 'entropy')
dt_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = dt_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = dt_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Decision Tree using Smote but without PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

dt_param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model_dt = DecisionTreeClassifier()

time_start = time.time()
model_dt, acc_dt, f1_dt, roc_auc_dt, param_dt = train_model_with_random_search(model_dt, dt_param_grid,
                                                                    X_train, y_train, X_test, y_test, cv=5)
time_taken_dt = time.time() - time_start
print('Best Parameters: ', param_dt)
print("Time Taken: ", round(time_taken_dt, 2), ' Seconds')

# Decision Tree Model
dt_model = DecisionTreeClassifier(splitter = 'best', min_samples_split= 2, min_samples_leaf= 1, max_depth= 30, criterion= 'entropy')
dt_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = dt_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = dt_model.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Decision Tree using Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

def plot_pca_variance(pca):
    plt.figure(figsize=(8, 6))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA: Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

n_components=17
pca=PCA().fit(X_scaled)
plot_pca_variance(pca)

print(f"Applying PCA with {n_components} components...")
pca = PCA(n_components=n_components, random_state=42)
X_pca = pca.fit_transform(X_scaled)
# Plot the PCA explained variance
plot_pca_variance(pca)
pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
pca_df = pd.DataFrame(X_pca, columns=pca_columns)
pca_df.head()

X_train, X_test, y_train, y_test = train_test_split(pca_df, y, test_size=0.2)

dt_param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model_dt = DecisionTreeClassifier()

time_start = time.time()
model_dt, acc_dt, f1_dt, roc_auc_dt, param_dt = train_model_with_random_search(model_dt, dt_param_grid,
                                                                    X_train, y_train, X_test, y_test, cv=5)
time_taken_dt = time.time() - time_start
print('Best Parameters: ', param_dt)
print("Time Taken: ", round(time_taken_dt, 2), ' Seconds')

# Decision Tree Model
dt_model = DecisionTreeClassifier(splitter = 'best', min_samples_split= 2, min_samples_leaf= 1, max_depth= 10, criterion= 'gini')
dt_model.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = dt_model.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = dt_model.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Logistic Regression Model without Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

log_reg_param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'liblinear'],
    'max_iter': [100, 200, 300, 400, 500]
}

model_lr = LogisticRegression()
time_start = time.time()

# Call the updated function for Logistic Regression
model_lr, acc_lr, f1_lr, roc_auc_lr, param_lr = train_model_with_random_search(model_lr, log_reg_param_grid,
                                                                     X_train, y_train, X_test, y_test, cv=5)

time_taken_lr = time.time() - time_start
print(f"Time Taken: {time_taken_lr:.2f} seconds")
print('Best Parameters: ', param_lr)
print("Time Taken: ", round(time_taken_lr, 2), ' Seconds')

# Logistic Regression Model

log_reg = LogisticRegression(solver ='newton-cg', penalty='l2', max_iter=500, C=0.01)
log_reg.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = log_reg.score(X_train, y_train)
print("Training Accuracy:", train_score)

y_pred = log_reg.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Logistic Regression Model using Smote & PCA"""

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

before_sampling = encoded_df.drop(['y'], axis=1).shape[0]
print(f"Number of rows before SMOTE: {before_sampling} \n")

# Counting the number of rows after SMOTE
after_sampling = X_train.shape[0] + X_test.shape[0]
print(f"Number of rows after SMOTE: {after_sampling} \n")

# Calculating the increase in number of rows
increase_in_rows = after_sampling - before_sampling
print(f"Increase in number of rows after SMOTE: {increase_in_rows}")

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

def plot_pca_variance(pca):
    plt.figure(figsize=(8, 6))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA: Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

n_components=17
pca=PCA().fit(X_scaled)
plot_pca_variance(pca)

print(f"Applying PCA with {n_components} components...")
pca = PCA(n_components=n_components, random_state=42)
X_pca = pca.fit_transform(X_scaled)
# Plot the PCA explained variance
plot_pca_variance(pca)
pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
pca_df = pd.DataFrame(X_pca, columns=pca_columns)
pca_df.head()

X_train, X_test, y_train, y_test = train_test_split(pca_df, y, test_size=0.2)

log_reg_param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'liblinear'],
    'max_iter': [100, 200, 300, 400, 500]
}

model_lr = LogisticRegression()
time_start = time.time()

# Call the updated function for Logistic Regression
model_lr, acc_lr, f1_lr, roc_auc_lr, param_lr = train_model_with_random_search(model_lr, log_reg_param_grid,
                                                                     X_train, y_train, X_test, y_test, cv=5)

time_taken_lr = time.time() - time_start
print(f"Time Taken: {time_taken_lr:.2f} seconds")
print('Best Parameters: ', param_lr)
print("Time Taken: ", round(time_taken_lr, 2), ' Seconds')

# Logistic Regression Model
log_reg= LogisticRegression(solver = 'liblinear', penalty = 'l1', max_iter = 400, C = 1)
log_reg.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = log_reg.score(X_train, y_train)
print("Training Accuracy:", train_score)

y_pred = log_reg.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Logistic Regression Model using Smote & without PCA"""

encoded_df = encoded_df.reset_index(drop=True)
# Define the number of folds for cross-validation
n_folds = 5

# Initialize lists to store the estimated parameters and validation scores from each fold
params = []
validation_scores = []

# Perform cross-validation
kf = KFold(n_splits=n_folds)
for train_index, test_index in kf.split(encoded_df):
    # Split the data into training and testing sets for each fold
    train = encoded_df.loc[train_index, :].copy()
    valid = encoded_df.loc[test_index, :].copy()

    response = 'y'
    predictors = encoded_df.columns.drop('y').tolist()

    X_train = train[predictors].values.astype('float64')
    y_train = train[response].values

    X_valid = valid[predictors].values.astype('float64')
    y_valid = valid[response].values

    # Convert the response variable to binary (assuming SalePrice > threshold indicates 1, else 0)
    threshold = train[response].median()  # Define a threshold for binary classification
    y_train_binary = (y_train > threshold).astype(int)
    y_valid_binary = (y_valid > threshold).astype(int)

    # Fit a Logistic Regression model
    model = LogisticRegression(solver='liblinear')  # Use solver 'liblinear' for small datasets
    result = model.fit(X_train, y_train_binary)

    # Store the estimated parameters for this fold
    params.append(model.coef_)

    # Compute the validation score (e.g., mean squared error or accuracy)
    y_pred = model.predict(X_valid)
    validation_score = mean_squared_error(y_valid_binary, y_pred)
    validation_scores.append(validation_score)

# Compute the mean of the estimated parameters across all folds
mean_params = np.mean(params, axis=0)

# Compute the mean of the validation scores across all folds
mean_validation_score = np.mean(validation_scores)

print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Print the average validation score
print(f"Mean Validation Score (MSE): {mean_validation_score}")

# Print the estimated parameters (coefficients) across all folds
print("Average Parameters (Coefficients):")
print(mean_params)

# Create a DataFrame for validation scores
validation_scores_df = pd.DataFrame({"Fold": range(1, n_folds + 1), "Validation Score": validation_scores})

# Print the validation scores table
print("\nValidation Scores:")
print(validation_scores_df)

# Custom Logistic Regression using MLE
class CustomLogisticRegression:
    def __init__(self, max_iter=200, tol=1e-4):
        self.max_iter = max_iter
        self.tol = tol
        self.coef_ = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def log_likelihood(self, coef, X, y):
        y_pred = self.sigmoid(X @ coef)
        ll = np.sum(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))
        return -ll  # Return negative log-likelihood for minimization

    def fit(self, X, y):
        # Add intercept to features
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term
        initial_coef = np.zeros(X.shape[1])  # Initial coefficients
        result = minimize(self.log_likelihood, initial_coef, args=(X, y), method='BFGS', options={'maxiter': self.max_iter})
        self.coef_ = result.x

    def predict_proba(self, X):
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term
        return self.sigmoid(X @ self.coef_)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

# Data Preparation
X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

# Applying SMOTE to the training data
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Combine the SMOTE-applied training data for scaling
X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)

print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Function to train the model and evaluate MLE-related metrics
def train_and_evaluate_model(X_train, y_train, X_test, y_test):
    model = CustomLogisticRegression(max_iter=200)
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    # Calculate metrics
    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    f1 = f1_score(y_test, y_pred)

    # Print results
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC Score: {roc_auc:.4f}")

    # Plot the ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.show()

# Evaluate the custom logistic regression model
train_and_evaluate_model(X_train, y_train, X_test, y_test)

# Cross-validation using sklearn's Logistic Regression for comparison
sklearn_log_reg = LogisticRegression(solver='liblinear', penalty='l1', max_iter=200, C=1)
sklearn_log_reg.fit(X_train, y_train)

# Accuracy of sklearn logistic regression model
cv_scores = cross_val_score(sklearn_log_reg, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Sklearn Logistic Regression Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = sklearn_log_reg.score(X_train, y_train)  # Training accuracy
print("Sklearn Training Accuracy:", train_score)

# Compare training score with mean CV score
if train_score > np.mean(cv_scores):
    print("The Sklearn model may be overfitting.")
else:
    print("The Sklearn model is not overfitting.")

y_pred = sklearn_log_reg.predict(X_test)
print("Sklearn Test Accuracy:", accuracy_score(y_test, y_pred))
print("Sklearn Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Sklearn Classification Report:\n", classification_report(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(encoded_df.drop(['y'], axis=1), encoded_df['y'], test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)

# Applying SMOTE on the training data
X_train, y_train = smote.fit_resample(X_train, y_train)

before_sampling = encoded_df.drop(['y'], axis=1).shape[0]
print(f"Number of rows before SMOTE: {before_sampling} \n")

# Counting the number of rows after SMOTE
after_sampling = X_train.shape[0] + X_test.shape[0]
print(f"Number of rows after SMOTE: {after_sampling} \n")

# Calculating the increase in number of rows
increase_in_rows = after_sampling - before_sampling
print(f"Increase in number of rows after SMOTE: {increase_in_rows}")

X = pd.concat([X_train, X_test], axis=0)
y = pd.concat([y_train, y_test], axis=0)
print("Standardizing the data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

log_reg_param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'liblinear'],
    'max_iter': [100, 200, 300, 400, 500]
}

model_lr = LogisticRegression()
time_start = time.time()

# Call the updated function for Logistic Regression
model_lr, acc_lr, f1_lr, roc_auc_lr, param_lr = train_model_with_random_search(model_lr, log_reg_param_grid,
                                                                     X_train, y_train, X_test, y_test, cv=5)

time_taken_lr = time.time() - time_start
print(f"Time Taken: {time_taken_lr:.2f} seconds")
print('Best Parameters: ', param_lr)
print("Time Taken: ", round(time_taken_lr, 2), ' Seconds')

# Logistic Regression Model
log_reg= LogisticRegression(solver = 'liblinear', penalty = 'l1', max_iter = 200, C = 1)
log_reg.fit(X_train, y_train)

# Accuracy of logistic regression model
cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print("Std CV Score:", np.std(cv_scores))

# To check for overfitting, let's fit the model on the full dataset and check accuracy
train_score = log_reg.score(X_train, y_train)  # Training accuracy
print("Training Accuracy:", train_score)

y_pred = log_reg.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Comparison of prediction using Smote without PCA"""

models = {
    'Logistic Regression': log_reg,
    'Decision Tree': dt_model,
    'MLP Classifier': mlp_model,
    'XGBoost': xgb_model
}

# Perform 5-fold cross-validation and collect scores
accuracies = {}
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    accuracies[name] = scores
    print(f"{name}: Mean CV Accuracy: {np.mean(scores):.4f}, Std Dev: {np.std(scores):.4f}")

# Prepare data for comparison
cv_scores_summary = pd.DataFrame({
    'Model': [],
    'Mean CV Score': [],
    'Standard Deviation': []
})

for name, scores in accuracies.items():
    cv_scores_summary = pd.concat([cv_scores_summary, pd.DataFrame({
        'Model': [name],
        'Mean CV Score': [np.mean(scores)],
        'Standard Deviation': [np.std(scores)]
    })], ignore_index=True)
# Print CV scores summary
print(cv_scores_summary)

plt.figure(figsize=(10, 7))

# Colors for each model's ROC curve
colors = ['blue', 'green', 'red', 'orange']

# Plot ROC curves for each model
for color, (name, model) in zip(colors, models.items()):
    # Get the probabilities for the positive class
    y_probs = model.predict_proba(X_test)[:, 1]

    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_probs)

    # Calculate AUC
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot the diagonal line (chance level)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

# Set the plot title and labels
plt.title('ROC Curves for All Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc='lower right')
plt.show()

# Create an empty DataFrame to store the metrics for each model
metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

# Loop through each model and calculate the metrics
for name, model in models.items():
    # Ensure `model` is the trained model object, not a string
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Append results to the DataFrame
    new_metrics = pd.DataFrame({
        'Model': [name],
        'Accuracy': [accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1-Score': [f1]
    })

    metrics_df = pd.concat([metrics_df, new_metrics], ignore_index=True)

# Display the metrics DataFrame
print(metrics_df)

# Continue with the plotting code...


# Extract metrics for plotting
accuracy = metrics_df['Accuracy']
precision = metrics_df['Precision']
recall = metrics_df['Recall']
f1_score = metrics_df['F1-Score']
models = metrics_df['Model']  # Ensure you reference the model names correctly

bar_width = 0.15
x = np.arange(len(models))

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

bars1 = ax.bar(x - 1.5 * bar_width, accuracy, width=bar_width, label='Accuracy', color='blue')
bars2 = ax.bar(x - 0.5 * bar_width, precision, width=bar_width, label='Precision', color='orange')
bars3 = ax.bar(x + 0.5 * bar_width, recall, width=bar_width, label='Recall', color='green')
bars4 = ax.bar(x + 1.5 * bar_width, f1_score, width=bar_width, label='F1-Score', color='red')

# Adding data labels
for bars in [bars1, bars2, bars3, bars4]:
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')

# Setting labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Comparison of Model Metrics for Subscription Prediction', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=15)

# Move the legend outside of the plot area
ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')

# Adding grid lines
ax.yaxis.grid(True)

plt.tight_layout()
plt.show()

"""Comparison of prediction using Smote & PCA"""

# Create an empty DataFrame to store the metrics for each model
metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

# Loop through each model and calculate the metrics
for name, model in models.items():
    # Ensure `model` is the trained model object, not a string
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Append results to the DataFrame
    new_metrics = pd.DataFrame({
        'Model': [name],
        'Accuracy': [accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1-Score': [f1]
    })

    metrics_df = pd.concat([metrics_df, new_metrics], ignore_index=True)

# Display the metrics DataFrame
print(metrics_df)

# Continue with the plotting code...


# Extract metrics for plotting
accuracy = metrics_df['Accuracy']
precision = metrics_df['Precision']
recall = metrics_df['Recall']
f1_score = metrics_df['F1-Score']
models = metrics_df['Model']  # Ensure you reference the model names correctly

bar_width = 0.15
x = np.arange(len(models))

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

bars1 = ax.bar(x - 1.5 * bar_width, accuracy, width=bar_width, label='Accuracy', color='blue')
bars2 = ax.bar(x - 0.5 * bar_width, precision, width=bar_width, label='Precision', color='orange')
bars3 = ax.bar(x + 0.5 * bar_width, recall, width=bar_width, label='Recall', color='green')
bars4 = ax.bar(x + 1.5 * bar_width, f1_score, width=bar_width, label='F1-Score', color='red')

# Adding data labels
for bars in [bars1, bars2, bars3, bars4]:
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')

# Setting labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Comparison of Model Metrics for Subscription Prediction', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=15)

# Move the legend outside of the plot area
ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')

# Adding grid lines
ax.yaxis.grid(True)

plt.tight_layout()
plt.show()